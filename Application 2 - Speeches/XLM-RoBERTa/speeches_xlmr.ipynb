{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 5.07M/5.07M [00:00<00:00, 6.44MB/s]\n",
      "Downloading: 100%|██████████| 9.10M/9.10M [00:01<00:00, 4.87MB/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1602 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       843 comments\n",
      "   Min length: 237 tokens\n",
      "   Max length: 33,857 tokens\n",
      "Median length: 2,800.0 tokens\n",
      "837 of 843 sentences (99.3%) in the training set are longer than 500 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joan/miniforge3/envs/python38/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.chdir(\"\")\n",
    "\n",
    "import helper_functions as hf\n",
    "\n",
    "data = pd.read_csv(\"merged_speeches_populism_v2.csv\") # use \n",
    "len(data)\n",
    "data[\"text\"] = [i.replace(\"\\n\", \" \") for i in data[\"text\"]]\n",
    "var = 'speechtype' # set main variable to be used for classification\n",
    "data[var].value_counts()\n",
    "data[\"label\"] = data[var].astype('category')\n",
    "data[\"label\"] = data[\"label\"].cat.codes\n",
    "data.columns\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data[\"label\"].value_counts()\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-large\", do_lower_case=True)\n",
    "\n",
    "input_ids = []\n",
    "lengths = []\n",
    "for x, row in data.iterrows():\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        row['text'],                      \n",
    "                        add_special_tokens = True,\n",
    "                   )\n",
    "    input_ids.append(encoded_sent)\n",
    "    lengths.append(len(encoded_sent))\n",
    "\n",
    "print('{:>10,} comments'.format(len(input_ids)))\n",
    "print('   Min length: {:,} tokens'.format(min(lengths)))\n",
    "print('   Max length: {:,} tokens'.format(max(lengths)))\n",
    "print('Median length: {:,} tokens'.format(np.median(lengths)))\n",
    "\n",
    "hf.plot_distribution(lengths)\n",
    "\n",
    "max_len = 512 #max(lengths)\n",
    "\n",
    "num_truncated = np.sum(np.greater(lengths, max_len))\n",
    "num_sentences = len(lengths)\n",
    "prcnt = float(num_truncated) / float(num_sentences)\n",
    "print('{:,} of {:,} sentences ({:.1%}) in the training set are longer than {:} tokens.'.format(num_truncated, num_sentences, prcnt, max_len))\n",
    "\n",
    "# create tokenized data\n",
    "labels = []\n",
    "input_ids = []\n",
    "attn_masks = []\n",
    "\n",
    "for x, row in data.iterrows():\n",
    "    encoded_dict = tokenizer.encode_plus(row['text'],\n",
    "                                              max_length=max_len, #see other code for how to set this\n",
    "                                              padding='max_length',\n",
    "                                              truncation=True,\n",
    "                                              return_tensors='pt')\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attn_masks.append(encoded_dict['attention_mask'])\n",
    "    labels.append(row['label'])\n",
    "\n",
    "\n",
    "# Convert into tensor matrix.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attn_masks = torch.cat(attn_masks, dim=0)\n",
    "\n",
    "# Labels list to tensor.\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Create TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attn_masks, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Specify key model parameters here: \n",
    "model_name = \"xlm-roberta-large\"\n",
    "lr = 5e-6\n",
    "epochs = 6\n",
    "batch_size = 16 # ~65GB with 500 tokens and 16 batch size. 32 batch size goes over RAM limit (A100, 80GB)\n",
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 6\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "torch.cuda.empty_cache() #Clear GPU cache if necessary\n",
    "\n",
    "training_stats = [] # Store training and validation loss,validation accuracy, and timings.\n",
    "fold_stats = []\n",
    "\n",
    "total_t0 = time.time() # Measure the total training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================== #\n",
    "#              CV Training                 #\n",
    "# ======================================== #\n",
    "  \n",
    "# repeat 10 times\n",
    "\n",
    "k_folds = 10\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H%M')\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "    # Print\n",
    "    print(f'FOLD {fold+1}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=batch_size, sampler=train_subsampler)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=batch_size, sampler=test_subsampler)\n",
    "    \n",
    "    # Initiate model parameters for each fold\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "    device = torch.device('cuda:0')\n",
    "    desc = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr = lr, eps = 1e-6) \n",
    "    total_steps = (int(len(dataset)/batch_size)+1) * epochs \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "          \n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0 # Reset the total loss for this epoch.\n",
    "        model.train() # Put the model into training mode.\n",
    "        update_interval = hf.good_update_interval( # Pick an interval on which to print progress updates.\n",
    "                    total_iters = len(train_dataloader),\n",
    "                    num_desired_updates = 10\n",
    "                )\n",
    "\n",
    "        predictions_t, true_labels_t = [], []\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if (step % update_interval) == 0 and not step == 0:\n",
    "                elapsed = hf.format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed), end='\\r')\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            # Always clear any previously calculated gradients before performing a backward pass.\n",
    "            model.zero_grad()\n",
    "            # Perform a forward pass --returns the loss and the \"logits\"\n",
    "            loss = model(b_input_ids,\n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels)[0]\n",
    "            logits = model(b_input_ids,\n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)[1]\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end.\n",
    "            total_train_loss += loss.item()\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "            # Clip the norm of the gradients to 1.0. This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            optimizer.step()\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "            \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            # Store predictions and true labels\n",
    "            predictions_t.append(logits)\n",
    "            true_labels_t.append(label_ids)\n",
    "\n",
    "        # Combine the results across all batches.\n",
    "        flat_predictions_t = np.concatenate(predictions_t, axis=0)\n",
    "        flat_true_labels_t = np.concatenate(true_labels_t, axis=0)\n",
    "        # For each sample, pick the label (0, 1) with the highest score.\n",
    "        predicted_labels_t = np.argmax(flat_predictions_t, axis=1).flatten()        \n",
    "        acc_t = accuracy_score(predicted_labels_t, flat_true_labels_t)\n",
    "        \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = hf.format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        print(\"  Training accuracy: {:.3f}\".format(acc_t))\n",
    "        \n",
    "        if acc_t > 0.9 and epoch_i >= 3:\n",
    "            break        \n",
    "\n",
    "    # TEST\n",
    "    # After the completion of each training epoch, measure our performance on our test set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running test...\")\n",
    "    t0 = time.time()\n",
    "    model.eval() # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "    total_eval_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            loss = model(b_input_ids,\n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels)[0]\n",
    "            logits = model(b_input_ids,\n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)[1]\n",
    "        # Accumulate the test loss.\n",
    "        total_eval_loss += loss.item()\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Store predictions and true labels\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    # Combine the results across all batches.\n",
    "    flat_predictions = np.concatenate(predictions, axis=0)\n",
    "    flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "    # For each sample, pick the label (0, 1) with the highest score.\n",
    "    predicted_labels = np.argmax(flat_predictions, axis=1).flatten()\n",
    "    # Calculate the test accuracy.\n",
    "    val_accuracy = (predicted_labels == flat_true_labels).mean()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(test_dataloader)\n",
    "    \n",
    "    ov_acc = [accuracy_score(predicted_labels, flat_true_labels), recall_score(predicted_labels, flat_true_labels, average=\"macro\"), precision_score(predicted_labels, flat_true_labels, average=\"macro\"),f1_score(predicted_labels, flat_true_labels, average=\"macro\")]\n",
    "    f1 = list(f1_score(flat_true_labels,predicted_labels,average=None))\n",
    "    matrix = confusion_matrix(flat_true_labels,predicted_labels)\n",
    "    acc = list(matrix.diagonal()/matrix.sum(axis=1))\n",
    "    cr = pd.DataFrame(classification_report(pd.Series(flat_true_labels),pd.Series(predicted_labels), output_dict=True)).transpose().iloc[0:4, 0:2]\n",
    "    prec =list(cr.iloc[:,0])\n",
    "    rec = list(cr.iloc[:,1]) \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"Campaign (0): {0:.3f}\".format(acc[0]))\n",
    "    print(\"Famous (1): {0:.3f}\".format(acc[1]))\n",
    "    print(\"International (2): {0:.3f}\".format(acc[2]))\n",
    "    print(\"Ribboncutting (3): {0:.3f}\".format(acc[3]))\n",
    "    print('XLM-RoBERTa Prediction accuracy: {:.3f}'.format(val_accuracy))\n",
    "    \n",
    "    # Measure how long the test run took.\n",
    "    test_time = hf.format_time(time.time() - t0)\n",
    "    print(\"  Test Loss: {0:.3f}\".format(avg_val_loss))\n",
    "    print(\"  Test took: {:}\".format(test_time))        \n",
    "\n",
    "    fold_stats.append(\n",
    "        {\n",
    "            'fold': fold+1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Test Loss': avg_val_loss,\n",
    "            'Test Accur.': ov_acc[0],\n",
    "            'Campaign (0) Accur.': acc[0],\n",
    "            'Famous (1) Accur.': acc[1],\n",
    "            'International (2) Accur.': acc[2],\n",
    "            'Ribboncutting (3) Accur.': acc[3],\n",
    "            'f1': [f1, ov_acc[3]],\n",
    "            'prec': [prec, ov_acc[2]],\n",
    "            'rec': [rec, ov_acc[1]]\n",
    "        }\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "speechtype_stats = []\n",
    "speechtype_stats.append(\n",
    "    {\n",
    "        'Model': model_name,\n",
    "        'lr': lr,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "\n",
    "        'Campaign_mean': np.mean([x['Campaign (0) Accur.'] for x in fold_stats ]),\n",
    "        'Campaign_mean_sd': np.std([x['Campaign (0) Accur.'] for x in fold_stats ]),\n",
    "        'Campaign_mean_f1': np.mean([x['f1'][0][0] for x in fold_stats ]),\n",
    "        'Campaign_mean_f1_sd': np.std([x['f1'][0][0] for x in fold_stats ]),\n",
    "        'Campaign_recall': np.mean([x['rec'][0][0] for x in fold_stats ]),\n",
    "        'Campaign_recall_sd': np.std([x['rec'][0][0] for x in fold_stats ]),\n",
    "        'Campaign_prec': np.mean([x['prec'][0][0] for x in fold_stats ]),\n",
    "        'Campaign_prec_sd': np.std([x['prec'][0][0] for x in fold_stats ]),\n",
    "        \n",
    "        'Famous_mean': np.mean([x['Famous (1) Accur.'] for x in fold_stats ]),\n",
    "        'Famous_mean_sd': np.std([x['Famous (1) Accur.'] for x in fold_stats ]),\n",
    "        'Famous_mean_f1': np.mean([x['f1'][0][1] for x in fold_stats ]),\n",
    "        'Famous_mean_f1_sd': np.std([x['f1'][0][1] for x in fold_stats ]),\n",
    "        'Famous_recall': np.mean([x['rec'][0][1] for x in fold_stats ]),\n",
    "        'Famous_recall_sd': np.std([x['rec'][0][1] for x in fold_stats ]),\n",
    "        'Famous_prec': np.mean([x['prec'][0][1] for x in fold_stats ]),\n",
    "        'Famous_prec_sd': np.std([x['prec'][0][1] for x in fold_stats ]),\n",
    "        \n",
    "        'International_mean': np.mean([x['International (2) Accur.'] for x in fold_stats ]),\n",
    "        'International_mean_sd': np.std([x['International (2) Accur.'] for x in fold_stats ]),\n",
    "        'International_mean_f1': np.mean([x['f1'][0][2] for x in fold_stats ]),\n",
    "        'International_mean_f1_sd': np.std([x['f1'][0][2] for x in fold_stats ]),\n",
    "        'International_recall': np.mean([x['rec'][0][2] for x in fold_stats ]),\n",
    "        'International_recall_sd': np.std([x['rec'][0][2] for x in fold_stats ]),\n",
    "        'International_prec': np.mean([x['prec'][0][2] for x in fold_stats ]),\n",
    "        'International_prec_sd': np.std([x['prec'][0][2] for x in fold_stats ]),\n",
    "        \n",
    "        'Ribboncutting_mean': np.mean([x['Ribboncutting (3) Accur.'] for x in fold_stats ]),\n",
    "        'Ribboncutting_mean_sd': np.std([x['Ribboncutting (3) Accur.'] for x in fold_stats ]),\n",
    "        'Ribboncutting_mean_f1': np.mean([x['f1'][0][3] for x in fold_stats ]),\n",
    "        'Ribboncutting_mean_f1_sd': np.std([x['f1'][0][3] for x in fold_stats ]),\n",
    "        'Ribboncutting_recall': np.mean([x['rec'][0][3] for x in fold_stats ]),\n",
    "        'Ribboncutting_recall_sd': np.std([x['rec'][0][3] for x in fold_stats ]),\n",
    "        'Ribboncutting_prec': np.mean([x['prec'][0][3] for x in fold_stats ]),\n",
    "        'Ribboncutting_prec_sd': np.std([x['prec'][0][3] for x in fold_stats ]),\n",
    "        \n",
    "        'overall_mean': np.mean([x['Test Accur.'] for x in fold_stats ]),\n",
    "        'overall_mean_sd': np.std([x['Test Accur.'] for x in fold_stats ]),\n",
    "        'overall_mean_f1': np.mean([x['f1'][1] for x in fold_stats ]),\n",
    "        'overall_mean_f1_sd': np.std([x['f1'][1] for x in fold_stats ]),\n",
    "        'overall_recall': np.mean([x['rec'][1] for x in fold_stats ]),\n",
    "        'overall_recall_sd': np.std([x['rec'][1] for x in fold_stats ]),\n",
    "        'overall_prec': np.mean([x['prec'][1] for x in fold_stats ]),\n",
    "        'overall_prec_sd': np.std([x['prec'][1] for x in fold_stats ]),\n",
    "    }\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('fold_stats_xlmr_' + timestamp + '.txt', 'w') as outfile:\n",
    "  json.dump(fold_stats, outfile)\n",
    "with open('speechtype_results_xlmr_' + timestamp + '.txt', 'w') as outfile:\n",
    "  json.dump(speechtype_stats, outfile)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b90a2c654d3c94538bb3e550bc0e2ab91d754e4e918a5ba9891867723ce3de5a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('python38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
