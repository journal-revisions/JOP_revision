{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     2,076 comments\n",
      "   Min length: 9 tokens\n",
      "   Max length: 147 tokens\n",
      "Median length: 40.0 tokens\n",
      "17 of 2,076 sentences (0.8%) in the training set are longer than 95 tokens.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.chdir(\"\")\n",
    "\n",
    "data = pd.read_excel(\"updated_coding_1219.xlsx\")\n",
    "\n",
    "import helper_functions as hf\n",
    "\n",
    "len(data)\n",
    "var = 'type' \n",
    "data[var].value_counts()\n",
    "data = data[[var, \"text\"]]\n",
    "data.loc[:, 'text'] = data['text'].str.replace('\\n', ' ')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data[\"label\"] = data[var].astype('category')\n",
    "data[\"label\"] = data[\"label\"].cat.codes\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "# Record the length of each sequence (in terms of BERT tokens).\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\", do_lower_case=True)\n",
    "\n",
    "input_ids = []\n",
    "lengths = []\n",
    "for x, row in data.iterrows():\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        row['text'],                      \n",
    "                        add_special_tokens = True,\n",
    "                   )\n",
    "    input_ids.append(encoded_sent)\n",
    "    lengths.append(len(encoded_sent))\n",
    "\n",
    "print('{:>10,} comments'.format(len(input_ids)))\n",
    "print('   Min length: {:,} tokens'.format(min(lengths)))\n",
    "print('   Max length: {:,} tokens'.format(max(lengths)))\n",
    "print('Median length: {:,} tokens'.format(np.median(lengths)))\n",
    "\n",
    "hf.plot_distribution(lengths)\n",
    "\n",
    "max_len = 95 #max(lengths)\n",
    "\n",
    "num_truncated = np.sum(np.greater(lengths, max_len))\n",
    "num_sentences = len(lengths)\n",
    "prcnt = float(num_truncated) / float(num_sentences)\n",
    "print('{:,} of {:,} sentences ({:.1%}) in the training set are longer than {:} tokens.'.format(num_truncated, num_sentences, prcnt, max_len))\n",
    "\n",
    "# create tokenized data\n",
    "labels = []\n",
    "input_ids = []\n",
    "attn_masks = []\n",
    "\n",
    "for x, row in data.iterrows():\n",
    "    encoded_dict = tokenizer.encode_plus(row['text'],\n",
    "                                              max_length=max_len, #see other code for how to set this\n",
    "                                              padding='max_length',\n",
    "                                              truncation=True,\n",
    "                                              return_tensors='pt')\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attn_masks.append(encoded_dict['attention_mask'])\n",
    "    labels.append(row['label'])\n",
    "\n",
    "\n",
    "# Convert into tensor matrix.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attn_masks = torch.cat(attn_masks, dim=0)\n",
    "\n",
    "# Labels list to tensor.\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Create TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attn_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Specify key model parameters here: \n",
    "model_name = \"bert-large-uncased\"\n",
    "lr = 3e-5\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 6\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "torch.cuda.empty_cache() #Clear GPU cache if necessary\n",
    "\n",
    "training_stats = [] # Store training and validation loss,validation accuracy, and timings.\n",
    "fold_stats = []\n",
    "\n",
    "total_t0 = time.time() # Measure the total training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152d66ab4533419686fe3660951f4e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/joan/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:58.\n",
      "  Average training loss: 0.619\n",
      "  Training epoch took: 0:01:01\n",
      "  Training accuracy: 0.707\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.570\n",
      "  Training epoch took: 0:01:02\n",
      "  Training accuracy: 0.725\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.410\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.832\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.260\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.914\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.146\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.965\n",
      "\n",
      "Running test...\n",
      "  0: 0.964\n",
      "  1: 0.500\n",
      "BERT Prediction accuracy: 0.870\n",
      "  Test Loss: 0.578\n",
      "  Test took: 0:00:03\n",
      "FOLD 1\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/joan/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.573\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.741\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.506\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.764\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.322\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.874\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.183\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.942\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.077\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.981\n",
      "\n",
      "Running test...\n",
      "  0: 0.968\n",
      "  1: 0.434\n",
      "BERT Prediction accuracy: 0.832\n",
      "  Test Loss: 0.869\n",
      "  Test took: 0:00:03\n",
      "FOLD 2\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/joan/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.596\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.726\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.535\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.744\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.378\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.833\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.201\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.931\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.087\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.978\n",
      "\n",
      "Running test...\n",
      "  0: 0.923\n",
      "  1: 0.453\n",
      "BERT Prediction accuracy: 0.803\n",
      "  Test Loss: 1.088\n",
      "  Test took: 0:00:03\n",
      "FOLD 3\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/joan/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.601\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.722\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.555\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.740\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.425\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.818\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.235\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.915\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.123\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.974\n",
      "\n",
      "Running test...\n",
      "  0: 0.894\n",
      "  1: 0.396\n",
      "BERT Prediction accuracy: 0.779\n",
      "  Test Loss: 0.911\n",
      "  Test took: 0:00:03\n",
      "FOLD 4\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/joan/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.562\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.741\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.454\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.814\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.288\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.904\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.199\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.945\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.126\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.970\n",
      "\n",
      "Running test...\n",
      "  0: 0.912\n",
      "  1: 0.433\n",
      "BERT Prediction accuracy: 0.774\n",
      "  Test Loss: 1.151\n",
      "  Test took: 0:00:03\n",
      "FOLD 5\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/joan/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch   110  of    117.    Elapsed: 0:00:59.\n",
      "  Average training loss: 0.535\n",
      "  Training epoch took: 0:01:03\n",
      "  Training accuracy: 0.749\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    90  of    117.    Elapsed: 0:00:49.\r"
     ]
    }
   ],
   "source": [
    "# ======================================== #\n",
    "#              CV Training                 #\n",
    "# ======================================== #\n",
    "\n",
    "# repeat 10 times \n",
    "\n",
    "k_folds = 10\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H%M')\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    \n",
    "    # Print\n",
    "    print(f'FOLD {fold+1}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "                      dataset, \n",
    "                      batch_size=batch_size, sampler=train_subsampler)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=batch_size, sampler=test_subsampler)\n",
    "    \n",
    "    # Initiate model parameters for each fold\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    device = torch.device('cuda:0')\n",
    "    desc = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr = lr, eps = 1e-6) \n",
    "    total_steps = (int(len(dataset)/batch_size)+1) * epochs \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 10, num_training_steps = total_steps)\n",
    "          \n",
    "    # Run the training loop for defined number of epochs\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0 # Reset the total loss for this epoch.\n",
    "        model.train() # Put the model into training mode.\n",
    "        update_interval = hf.good_update_interval( # Pick an interval on which to print progress updates.\n",
    "                    total_iters = len(train_dataloader),\n",
    "                    num_desired_updates = 10\n",
    "                )\n",
    "\n",
    "        predictions_t, true_labels_t = [], []\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if (step % update_interval) == 0 and not step == 0:\n",
    "                elapsed = hf.format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed), end='\\r')\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            # Always clear any previously calculated gradients before performing a backward pass.\n",
    "            model.zero_grad()\n",
    "            # Perform a forward pass --returns the loss and the \"logits\"\n",
    "            loss = model(b_input_ids,\n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels)[0]\n",
    "            logits = model(b_input_ids,\n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)[1]\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end.\n",
    "            total_train_loss += loss.item()\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "            # Clip the norm of the gradients to 1.0. This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            optimizer.step()\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "            \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            # Store predictions and true labels\n",
    "            predictions_t.append(logits)\n",
    "            true_labels_t.append(label_ids)\n",
    "\n",
    "        # Combine the results across all batches.\n",
    "        flat_predictions_t = np.concatenate(predictions_t, axis=0)\n",
    "        flat_true_labels_t = np.concatenate(true_labels_t, axis=0)\n",
    "        # For each sample, pick the label (0, 1) with the highest score.\n",
    "        predicted_labels_t = np.argmax(flat_predictions_t, axis=1).flatten()        \n",
    "        acc_t = accuracy_score(predicted_labels_t, flat_true_labels_t)\n",
    "        \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = hf.format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        print(\"  Training accuracy: {:.3f}\".format(acc_t))\n",
    "        \n",
    "        if acc_t > 0.9 and epoch_i >= 3:\n",
    "            break\n",
    "\n",
    "    # TEST\n",
    "    # After the completion of each training epoch, measure our performance on our test set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running test...\")\n",
    "    t0 = time.time()\n",
    "    model.eval() # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "    total_eval_loss = 0\n",
    "    predictions, true_labels = [], []\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            loss = model(b_input_ids,\n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels)[0]\n",
    "            logits = model(b_input_ids,\n",
    "                                attention_mask=b_input_mask,\n",
    "                                labels=b_labels)[1]\n",
    "        # Accumulate the test loss.\n",
    "        total_eval_loss += loss.item()\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Store predictions and true labels\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    # Combine the results across all batches.\n",
    "    flat_predictions = np.concatenate(predictions, axis=0)\n",
    "    flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "    # For each sample, pick the label (0, 1) with the highest score.\n",
    "    predicted_labels = np.argmax(flat_predictions, axis=1).flatten()\n",
    "    # Calculate the test accuracy.\n",
    "    val_accuracy = (predicted_labels == flat_true_labels).mean()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(test_dataloader)\n",
    "    \n",
    "    ov_acc = [accuracy_score(predicted_labels, flat_true_labels), recall_score(predicted_labels, flat_true_labels, average=\"macro\"), precision_score(predicted_labels, flat_true_labels, average=\"macro\"),f1_score(predicted_labels, flat_true_labels, average=\"macro\")]\n",
    "    f1 = list(f1_score(flat_true_labels,predicted_labels,average=None))\n",
    "    matrix = confusion_matrix(flat_true_labels,predicted_labels)\n",
    "    acc = list(matrix.diagonal()/matrix.sum(axis=1))\n",
    "    cr = pd.DataFrame(classification_report(pd.Series(flat_true_labels),pd.Series(predicted_labels), output_dict=True)).transpose().iloc[0:3, 0:2]\n",
    "    prec =list(cr.iloc[:,0])\n",
    "    rec = list(cr.iloc[:,1]) \n",
    "\n",
    "    # Report the final accuracy for this test run.\n",
    "    print(\"  0: {0:.3f}\".format(acc[0]))\n",
    "    print(\"  1: {0:.3f}\".format(acc[1]))\n",
    "    print('BERT Prediction accuracy: {:.3f}'.format(val_accuracy))\n",
    "    \n",
    "    # Measure how long the test run took.\n",
    "    test_time = hf.format_time(time.time() - t0)\n",
    "    print(\"  Test Loss: {0:.3f}\".format(avg_val_loss))\n",
    "    print(\"  Test took: {:}\".format(test_time))        \n",
    "\n",
    "    fold_stats.append(\n",
    "        {\n",
    "            'fold': fold+1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Test Loss': avg_val_loss,\n",
    "            'Test Accur.': ov_acc[0],\n",
    "            '0 Accur.': acc[0],\n",
    "            '1 Accur.': acc[1],\n",
    "            'f1': [f1, ov_acc[3]],\n",
    "            'prec': [prec, ov_acc[2]],\n",
    "            'rec': [rec, ov_acc[1]]\n",
    "        }\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "civility_stats = []\n",
    "civility_stats.append(\n",
    "    {\n",
    "        'Model': model_name,\n",
    "        'lr': lr,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        \n",
    "        'civil_mean': np.mean([x['0 Accur.'] for x in fold_stats ]),\n",
    "        'civil_mean_sd': np.std([x['0 Accur.'] for x in fold_stats ]),\n",
    "        'civil_mean_f1': np.mean([x['f1'][0][0] for x in fold_stats ]),\n",
    "        'civil_mean_f1_sd': np.std([x['f1'][0][0] for x in fold_stats ]),\n",
    "        'civil_recall': np.mean([x['rec'][0][0] for x in fold_stats ]),\n",
    "        'civil_recall_sd': np.std([x['rec'][0][0] for x in fold_stats ]),\n",
    "        'civil_prec': np.mean([x['prec'][0][0] for x in fold_stats ]),\n",
    "        'civil_prec_sd': np.std([x['prec'][0][0] for x in fold_stats ]),\n",
    "        \n",
    "        'incivil_mean': np.mean([x['1 Accur.'] for x in fold_stats ]),\n",
    "        'incivil_mean_sd': np.std([x['1 Accur.'] for x in fold_stats ]),\n",
    "        'incivil_mean_f1': np.mean([x['f1'][0][1] for x in fold_stats ]),\n",
    "        'incivil_mean_f1_sd': np.std([x['f1'][0][1] for x in fold_stats ]),\n",
    "        'incivil_recall': np.mean([x['rec'][0][1] for x in fold_stats ]),\n",
    "        'incivil_recall_sd': np.std([x['rec'][0][1] for x in fold_stats ]),\n",
    "        'incivil_prec': np.mean([x['prec'][0][1] for x in fold_stats ]),\n",
    "        'incivil_prec_sd': np.std([x['prec'][0][1] for x in fold_stats ]),\n",
    "        \n",
    "        'overall_mean': np.mean([x['Test Accur.'] for x in fold_stats ]),\n",
    "        'overall_mean_sd': np.std([x['Test Accur.'] for x in fold_stats ]),\n",
    "        'overall_mean_f1': np.mean([x['f1'][1] for x in fold_stats ]),\n",
    "        'overall_mean_f1_sd': np.std([x['f1'][1] for x in fold_stats ]),\n",
    "        'overall_recall': np.mean([x['rec'][1] for x in fold_stats ]),\n",
    "        'overall_recall_sd': np.std([x['rec'][1] for x in fold_stats ]),\n",
    "        'overall_prec': np.mean([x['prec'][1] for x in fold_stats ]),\n",
    "        'overall_prec_sd': np.std([x['prec'][1] for x in fold_stats ]),\n",
    "\n",
    "        'f1_0': [f['f1'][0][0] for f in fold_stats],\n",
    "        'f1_1': [f['f1'][0][1] for f in fold_stats],\n",
    "    }\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('fold_stats_' + timestamp + '.txt', 'w') as outfile:\n",
    "  json.dump(fold_stats, outfile)\n",
    "with open('civility_bert_' + timestamp + '.txt', 'w') as outfile:\n",
    "  json.dump(civility_stats, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
